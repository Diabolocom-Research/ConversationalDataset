import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import zipfile
import io
import shutil
import traceback
import json
import time
from rich.progress import track


class TalkBankDownloader(object):
	def __init__(self):
		self.df_banks = self._hardcoded_available_banks()
		self.df_datasets = {} # {'bank_name (str)': 'df (pd.DataFrame)'}
		self.df_sub_datasets = {} # {'bank_name/dataset_name (str)': 'df (pd.DataFrame)'}

	def _hardcoded_available_banks(self):
		"""
		Hardcoded list of available banks, because the access differs
		Return:
			df (pd.DataFrame): list of available banks
		"""
		data = {
			"bank_name": [
				"ca",
				"samtale",
				"class",
			],
			"bank_url": [
				"https://ca.talkbank.org/access/",
				"https://samtale.talkbank.org/",
				"https://class.talkbank.org/access/",
			]
		}
		df = pd.DataFrame(data=data)
		return df

	def get_banks(self):
		"""
		Hardcoded list of available banks, because the access differs
		(see _hardcoded_available_banks)
		Return:
			df (pd.DataFrame): list of available banks
		"""
		return self.df_banks
	
	def _clean_df_get_datasets(self, df):
		"""
		Util function for 'get_datasets' function to clean the dataset generated by read_html
		"""
		df.dropna(inplace=True)
		df.reset_index(drop=True, inplace=True)
		for col in df.columns:
			df[col] = df[col].apply(lambda x: " ".join(x.split()))
		df.columns = [" ".join(x.lower().split()) for x in df.columns]
		df.rename(columns={"corpus": "dataset_name"}, inplace=True)
		i = df[df["dataset_name"] == "Collection"].index
		if len(i) > 0:
			df = df.head(i[0])
		if "rating" in df.columns:
			df.loc[:, "rating"] = df["rating"].apply(lambda x: len(x))    
		return df

	def _match_bank(self, bank):
		"""
		Util function to match (bank_name | bank_url) to an existing bank
		Raise a ValueError if no match found
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
		Return:
			bank_name (str): bank name
			bank_url (str): bank url
		"""
		df_tmp = self.df_banks[(self.df_banks["bank_name"] == bank) | (self.df_banks["bank_url"] == bank)]
		if df_tmp.shape[0] != 1:
			raise ValueError(f"Param bank is not valid: '{bank}'. It must be either a bank_name or a bank_url (eg 'ca').")
		bank_name = df_tmp["bank_name"].values[0]
		bank_url = df_tmp["bank_url"].values[0]
		return bank_name, bank_url

	def get_datasets(self, bank):
		"""
		Scrap the talkbank website to list available datasets for a specific bank
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
		Return:
			df (pd.DataFrame): list of available datasets for this bank
		"""
		bank_name, bank_url = self._match_bank(bank)
		if bank_name in self.df_datasets:
			return self.df_datasets[bank_name]
		r = requests.get(bank_url)
		soup = BeautifulSoup(r.text, "html.parser")
		table = soup.find_all("table")[1]
		df = pd.read_html(io.StringIO(str(table)), header=0)[0]
		df = self._clean_df_get_datasets(df)
		links = []
		for i, row in enumerate(table.find_all("tr")):
			if i == 0:
				continue # skip headers
			if "collection" in row.find("td").text.lower():
				break # break at collection
			a = row.find("a")
			if a is None:
				continue # skip if no link
			link = bank_url + a["href"]
			links.append(link)
		df["dataset_url"] = links
		df["bank_name"] = bank_name
		df["bank_url"] = bank_url
		self.df_datasets[bank_name] = df
		return df

	def _match_dataset(self, bank, dataset):
		"""
		Util function to match (bank_name | bank_url) & (dataset_name | dataset_url) to an existing dataset
		Raise a ValueError if no match found
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
			dataset (str): dataset parameter can either be dataset_name (eg 'Bergmann') or dataset_url (eg 'https://ca.talkbank.org/access/Bergmann/')
		Return:
			bank_name (str): bank name
			bank_url (str): bank url
			dataset_name (str): dataset name
			dataset_url (str): dataset url
		"""
		bank_name, bank_url = self._match_bank(bank)
		if bank_name in self.df_datasets:
			df_dataset = self.df_datasets[bank_name]
		else:
			df_dataset = self.get_datasets(bank_name)
		df_tmp = df_dataset[(df_dataset["dataset_name"] == dataset) | (df_dataset["dataset_url"] == dataset)]
		if df_tmp.shape[0] != 1:
			raise ValueError(f"Param dataset is not valid: '{dataset}'. It must be either a dataset_name or a dataset_url (eg 'Bergmann').")
		dataset_name = df_tmp["dataset_name"].values[0]
		dataset_url = df_tmp["dataset_url"].values[0]
		return bank_name, bank_url, dataset_name, dataset_url

	def get_sub_datasets(self, bank, dataset):
		"""
		Scrap the TalkBank website to list all sub datasets for a given dataset
		If there is no sub dataset, return an empty pd.DataFrame
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
			dataset (str): dataset parameter can either be dataset_name (eg 'Bergmann') or dataset_url (eg 'https://ca.talkbank.org/access/Bergmann/')
		Return:
			df (pd.DataFrame): list of available sub datasets for this dataset
		"""
		bank_name, bank_url, dataset_name, dataset_url = self._match_dataset(bank, dataset)
		if "/".join([bank_name, dataset_name]) in self.df_sub_datasets:
			return self.df_sub_datasets["/".join([bank_name, dataset_name])]
		if dataset_url.endswith(".html"):
			# no sub datasets
			df = pd.DataFrame()
			self.df_sub_datasets["/".join([bank_name, dataset_name])] = df
			return df
		r = requests.get(dataset_url)
		soup = BeautifulSoup(r.text, "html.parser")
		table = soup.find_all("table")[1]
		df = pd.read_html(io.StringIO(str(table)), header=0)[0]
		df = self._clean_df_get_datasets(df)
		df.rename(columns={"dataset_name": "sub_dataset_name"}, inplace=True)
		links = []
		for i, row in enumerate(table.find_all("tr")):
			if i == 0:
				continue # skip headers
			if "collection" in row.find("td").text.lower():
				break # break at collection
			a = row.find("a")
			if a is None:
				continue # skip if no link
			if not dataset_url.endswith("/"):
				a["href"] = "/" + a["href"]
			link = dataset_url + a["href"]
			links.append(link)
		df["sub_dataset_url"] = links
		df["bank_name"] = bank_name
		df["bank_url"] = bank_url
		df["dataset_name"] = dataset_name
		df["dataset_url"] = dataset_url
		self.df_sub_datasets["/".join([bank_name, dataset_name])] = df
		return df

	def _match_sub_dataset(self, bank, dataset, sub_dataset):
		"""
		Util function to match (bank_name | bank_url) & (dataset_name | dataset_url) & (sub_dataset_name | sub_dataset_url) to an existing sub_dataset
		Raise a ValueError if no match found
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
			dataset (str): dataset parameter can either be dataset_name (eg 'Bergmann') or dataset_url (eg 'https://ca.talkbank.org/access/Bergmann/')
			sub_dataset (str): sub_dataset parameter can either be sub_dataset_name (eg 'English (N)') or sub_dataset_url (eg 'https://ca.talkbank.org/access/CallFriendeng-n.html')
		Return:
			bank_name (str): bank name
			bank_url (str): bank url
			dataset_name (str): dataset name
			dataset_url (str): dataset url
			sub_dataset_name (str): dataset name
			sub_dataset_url (str): dataset url
		"""
		bank_name, bank_url, dataset_name, dataset_url = self._match_dataset(bank, dataset)
		if not isinstance(sub_dataset, str):
			return bank_name, bank_url, dataset_name, dataset_url, None, None
		if "/".join([bank_name, dataset_name]) in self.df_sub_datasets:
			df_sub_datasets = self.df_sub_datasets["/".join([bank_name, dataset_name])]
		else:
			df_sub_datasets = self.get_sub_datasets(bank_name, dataset)
		df_tmp = df_sub_datasets[(df_sub_datasets["sub_dataset_name"] == sub_dataset) | (df_sub_datasets["sub_dataset_url"] == sub_dataset)]
		if df_tmp.shape[0] != 1:
			raise ValueError(f"Param sub_dataset is not valid: '{sub_dataset}'. It must be either a sub_dataset_name or a sub_dataset_url (eg 'English (N)').")
		sub_dataset_name = df_tmp["sub_dataset_name"].values[0]
		sub_dataset_url = df_tmp["sub_dataset_url"].values[0]
		return bank_name, bank_url, dataset_name, dataset_url, sub_dataset_name, sub_dataset_url

	def download_transcripts(self, s, transcripts_url, output_path):
		"""
		Download the zip on the passed url and extract the files in output_path
		Params:
			transcripts_url (str): url to download the trancripts
			output_path (str): path to directory to save transcripts
		Return:
			filenames (list str): list of new file path
		"""
		r = s.get(transcripts_url, stream=True)
		z = zipfile.ZipFile(io.BytesIO(r.content))
		filenames = []
		for member in z.namelist():
			if not member.endswith(".cha"):
				continue
			filename = os.path.normpath(member)
			filename = filename.split(os.sep)[1:]
			filename = os.path.join(output_path, *filename)
			dir_path = os.path.dirname(filename)
			if not os.path.isdir(dir_path):
				os.makedirs(dir_path)
			# copy file (taken from zipfile's extract)
			source = z.open(member)
			target = open(filename, "wb")
			with source, target:
				shutil.copyfileobj(source, target)
			source.close()
			target.close()
			filenames.append(filename)
		z.close()
		return filenames

	def download_media_recursive(self, s, media_url, output_path, overwrite=True):
		cookies = {
		    '_ga_904G144P96': 'GS1.2.1704725711.3.0.1704725711.0.0.0',
		    '_ga_L4L4XL5BVW': 'GS1.2.1713968432.3.0.1713968432.0.0.0',
		    'sc_is_visitor_unique': 'rx11702701.1715695524.003034389D9C4F7301EBB13DB259AA54.30.28.23.21.18.18.15.13.9',
		    '_ga_BLQ5NXZFG9': 'GS1.2.1715695524.25.0.1715695524.0.0.0',
		    '_ga_NY193TB781': 'GS1.1.1715695526.23.0.1715695526.0.0.0',
		    '_ga': 'GA1.1.2052482003.1704703670',
		    'talkbank': 's%3AqLXMpRgno_ljuPSW_ABjQzLyb0ja4dIK.0fRnANKIsYHpKGeLG0cBfDGSd4GIqCiNUWTdob24J6E',
		}
		headers = {
		    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
		    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
		    'Connection': 'keep-alive',
		    'If-None-Match': 'W/"1e2-mvhbTarIX/7QYKP/5s1O5ovlbCo"',
		    'Referer': 'https://media.talkbank.org:3000/fileListing?&bp=media&path=ca/CallFriend/fra-q',
		    'Sec-Fetch-Dest': 'document',
		    'Sec-Fetch-Mode': 'navigate',
		    'Sec-Fetch-Site': 'same-origin',
		    'Sec-Fetch-User': '?1',
		    'Upgrade-Insecure-Requests': '1',
		    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
		    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
		    'sec-ch-ua-mobile': '?0',
		    'sec-ch-ua-platform': '"Windows"',
		}
		r = s.get(media_url, cookies=cookies, headers=headers)
		soup = BeautifulSoup(r.text, "html.parser")
		if not media_url.endswith("/"):
			media_url += "/"
		exclude_names = ["name", "last modified", "size", "description", "parent directory"]
		links = [a["href"] for a in soup.find_all("a", href=True) if a.text.lower() not in exclude_names]
		name = os.path.basename(media_url[:-1]) if media_url.endswith("/") else os.path.basename(media_url)
		nlinks = []
		for link in links:
			link = link.split("/")[-1]
			if "." not in link:
				# is a directory
				self.download_media_recursive(s, media_url + link, os.path.join(output_path, link), overwrite=overwrite)
			elif "&type=save" not in link and\
			os.path.isfile(os.path.join(output_path, link.split(".")[0] + ".cha")) and\
			(not os.path.isfile(os.path.join(output_path, link)) or overwrite):
				# is a file
				nlinks.append(link)
		if len(nlinks) == 0:
			return
		if not os.path.isdir(output_path):
			os.makedirs(output_path)
		for link in track(nlinks, description=f"[cyan]Downloading {name}"):
			url = media_url.replace("fileListing?", "getFile?") + link + "&type=save"
			r = s.get(url, cookies=cookies, headers=headers)
			with open(os.path.join(output_path, link), 'wb') as f:
				f.write(r.content)

	def download_media(self, s, media_url, output_path, overwrite=True):
		"""
		Download all folder and subfolders like this one: https://media.talkbank.org/ca/CABNC/
		or this one: https://media.talkbank.org/ca/CallFriend/
		or this one: https://media.talkbank.org/ca/CallFriend/fra-q/
		Params:
			media_url (str): url to media (eg 'https://media.talkbank.org/ca/CallFriend/fra-q/')
			output_path (str): path to directory to save media
			skip_folder_startswith_0 (bool): pass True if you want to skip folders starting with a '0'
			overwrite (bool): pass True if you want to overwrite files
		"""
		self.download_media_recursive(s, media_url, output_path, overwrite=overwrite)

	def login_session(self):
		s = requests.Session()
		cookies = {
		    '_ga_904G144P96': 'GS1.2.1704725711.3.0.1704725711.0.0.0',
		    '_ga_L4L4XL5BVW': 'GS1.2.1713968432.3.0.1713968432.0.0.0',
		    'sc_is_visitor_unique': 'rx11702701.1715695524.003034389D9C4F7301EBB13DB259AA54.30.28.23.21.18.18.15.13.9',
		    '_ga_BLQ5NXZFG9': 'GS1.2.1715695524.25.0.1715695524.0.0.0',
		    '_ga_NY193TB781': 'GS1.1.1715695526.23.0.1715695526.0.0.0',
		    '_ga': 'GA1.1.2052482003.1704703670',
		    'talkbank': 's%3AqLXMpRgno_ljuPSW_ABjQzLyb0ja4dIK.0fRnANKIsYHpKGeLG0cBfDGSd4GIqCiNUWTdob24J6E',
		}
		headers = {
		    'Accept': '*/*',
		    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
		    'Connection': 'keep-alive',	
		    'Content-type': 'application/json',
		    'Origin': 'https://media.talkbank.org:3000',
		    'Referer': 'https://media.talkbank.org:3000/',
		    'Sec-Fetch-Dest': 'empty',
		    'Sec-Fetch-Mode': 'cors',
		    'Sec-Fetch-Site': 'same-site',
		    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
		    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
		    'sec-ch-ua-mobile': '?0',
		    'sec-ch-ua-platform': '"Windows"',
		}
		json_data = {
		    'email': 'diabolocom.rnd@gmail.com',
		    'pswd': 'DiAbOlOcOm_psswrd#42',
		}
		p = s.post('https://sla2.talkbank.org:4000/logInUser', cookies=cookies, headers=headers, json=json_data)
		return s

	def download_dataset(self, bank, dataset, sub_dataset=None, output_path=None, overwrite=True):
		"""
		Download a specific dataset. You need to select a valid bank / dataset / sub_dataset
		If the dataset doesn't contain sub_dataset you can pass sub_dataset=None
		Example of valid parameters:
			bank="ca", dataset="Bergmann", sub_dataset=None
			bank="ca", dataset="CallFriend", sub_dataset="English (N)"
		Params:
			bank (str): bank parameter can either be bank_name (eg 'ca') or bank_url (eg 'https://ca.talkbank.org/access/')
			dataset (str): dataset parameter can either be dataset_name (eg 'Bergmann') or dataset_url (eg 'https://ca.talkbank.org/access/Bergmann/')
			sub_dataset (str): sub_dataset parameter can either be sub_dataset_name (eg 'English (N)') or sub_dataset_url (eg 'https://ca.talkbank.org/access/CallFriendeng-n.html')
			output_path (str): Optional parameter if you want to specify the location, by default it creates a directory bank_name/dataset_name
²			overwrite (bool): pass True if you want to overwrite the files
		"""
		bank_name, bank_url, dataset_name, dataset_url, sub_dataset_name, sub_dataset_url = self._match_sub_dataset(bank, dataset, sub_dataset)
		if isinstance(sub_dataset_url, str):
			url = sub_dataset_url
			if output_path is None:
				output_path = os.path.join(bank_name, dataset_name, sub_dataset_name)
		else:
			url = dataset_url
			if output_path is None:
				output_path = os.path.join(bank_name, dataset_name)
		if not url.endswith(".html"):
			raise ValueError("Selected dataset contains sub datasets. Please select a sub_dataset.")
		if not os.path.isdir(output_path):
			os.makedirs(output_path)
		print("Saving data into directory:", output_path)

		r = requests.get(url)
		soup = BeautifulSoup(r.text, "html.parser")
		transcripts_url, media_url = None, None
		for link in soup.find_all("a", href=True):
			text = " ".join(link.text.lower().split())
			if "download transcripts" in text:
				# transcripts_url = url.split(".org")[0] + ".org" + link["href"]
				transcripts_url = "https://git.talkbank.org:3000/getFile?type=open&bp=data&path=ca" + link["href"]
			elif "media folder" in text:
				media_url = "https://media.talkbank.org:3000/fileListing?&bp=media&path=/" + "/".join(link["href"].replace("https://", "").split("/")[1:])
			if transcripts_url is not None and media_url is not None:
				break
		if transcripts_url in [None, 'None'] or media_url in [None, 'None']:
			print(f"{dataset_name} can't be downloaded\ntranscripts_url: {transcripts_url}\nmedia_url: {media_url}")
			return
			
		s = self.login_session()
		try:
			self.download_transcripts(s, transcripts_url, output_path)
			self.download_media(s, media_url, output_path, overwrite=overwrite)
		except:
			print(traceback.format_exc())
			print("Error while downloading")
			return
		s.close()
		return output_path


if __name__ == '__main__':
	from IPython.display import display
	import json


	tb_download = TalkBankDownloader()
	
	""" SEE AVAILABLE DATA """
	# df_banks = tb_download.get_banks()
	# display(df_banks[["bank_name", "bank_url"]].head())

	# BANK_NAME = "ca" # example
	# df_datasets = tb_download.get_datasets(BANK_NAME)
	# display(df_datasets[["dataset_name", "dataset_url", "description"]].head())

	# DATASET_NAME = "CallFriend" # example
	# df_sub_datasets = tb_download.get_sub_datasets(BANK_NAME, DATASET_NAME)
	# display(df_sub_datasets[["sub_dataset_name", "sub_dataset_url", "description"]].head())

	""" DOWNLOAD DATA """
	# cfg = {
	# 	"bank": "ca",
	# 	"dataset": "CallFriend",
	# 	"sub_dataset": "English (N)",
	# } # example

	cfg = {
		"bank": "ca",
		"dataset": "CallFriend",
		"sub_dataset": "French - Quebecois"
	}
	output_path = tb_download.download_dataset(**cfg)
